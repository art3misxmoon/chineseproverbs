{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D6RvW_oaL9m"
      },
      "source": [
        "#Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mju1aLzd3IB"
      },
      "source": [
        "####Move to Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "collapsed": true,
        "id": "XHAZkSngauLf",
        "outputId": "1f9aea1a-6da1-4a07-bb49-43092eb0e5fe"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1448349703.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Mount Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Repo info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "#moving to repo\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Repo info\n",
        "MYDRIVE = \"/content/drive/MyDrive\"\n",
        "REPO_NAME = \"chineseproverbs\"\n",
        "REPO_PATH = os.path.join(MYDRIVE, REPO_NAME)\n",
        "\n",
        "# Go to MyDrive\n",
        "os.chdir(MYDRIVE)\n",
        "\n",
        "# Clone if missing, else pull\n",
        "if not os.path.exists(REPO_PATH):\n",
        "    print(\"Cloning repo...\")\n",
        "    !git clone https://github.com/art3misxmoon/chineseproverbs.git\n",
        "else:\n",
        "    print(\"Repo exists, pulling latest updates...\")\n",
        "    os.chdir(REPO_PATH)\n",
        "    !git pull\n",
        "\n",
        "# Move to repo folder\n",
        "os.chdir(REPO_PATH)\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "!ls\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtAtM4susL66"
      },
      "source": [
        "##General Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mpd0HYSsQxh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "\n",
        "# --- Paths to your split files ---\n",
        "part1 = \"UNv1.0.en-zh.tar.gz.00\"\n",
        "part2 = \"UNv1.0.en-zh.tar.gz.01\"\n",
        "\n",
        "# --- Path for combined archive ---\n",
        "combined_tar = \"UNv1.0.en-zh.tar.gz\"\n",
        "\n",
        "# --- Concatenate the split files ---\n",
        "os.system(f\"cat {part1} {part2} > {combined_tar}\")\n",
        "print(f\"Combined archive saved to: {combined_tar}\")\n",
        "\n",
        "# --- Inspect contents of the tar.gz without extracting ---\n",
        "with tarfile.open(combined_tar, 'r:gz') as tar:\n",
        "    print(\"Files inside the combined tar.gz:\")\n",
        "    for member in tar.getmembers()[:20]:  # just show first 20 files\n",
        "        print(member.name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_huBz90nwnOP"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "import os\n",
        "\n",
        "tar_path = \"UNv1.0.en-zh.tar.gz\"\n",
        "extract_path = \"UNv1.0_en-zh\"\n",
        "\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "    tar.extractall(path=extract_path)\n",
        "\n",
        "print(\"Extraction complete. Files:\")\n",
        "print(os.listdir(extract_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yt8EJwqux-Vq"
      },
      "outputs": [],
      "source": [
        "ch_file = os.path.join(extract_path, \"en-zh\", \"UNv1.0.en-zh.zh\")\n",
        "en_file = os.path.join(extract_path, \"en-zh\", \"UNv1.0.en-zh.en\")\n",
        "\n",
        "# Peek at first 5 sentences\n",
        "with open(ch_file, \"r\", encoding=\"utf-8\") as f_ch, open(en_file, \"r\", encoding=\"utf-8\") as f_en:\n",
        "    for i, (c, e) in enumerate(zip(f_ch, f_en)):\n",
        "        if i >= 5:\n",
        "            break\n",
        "        print(f\"CH: {c.strip()}\")\n",
        "        print(f\"EN: {e.strip()}\")\n",
        "        print(\"---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snUEUYfnyHkF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "with open(ch_file, \"r\", encoding=\"utf-8\") as f_ch, open(en_file, \"r\", encoding=\"utf-8\") as f_en:\n",
        "    ch_lines = [line.strip() for line in f_ch]\n",
        "    en_lines = [line.strip() for line in f_en]\n",
        "\n",
        "df_un = pd.DataFrame({\n",
        "    \"chinese\": ch_lines,\n",
        "    \"english\": en_lines\n",
        "})\n",
        "\n",
        "print(df_un.head())\n",
        "print(f\"Total sentence pairs: {len(df_un)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjPCGrXhy-Kh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import tarfile\n",
        "import random\n",
        "\n",
        "# --- Paths ---\n",
        "tar_path = \"UNv1.0.en-zh.tar.gz\"\n",
        "extract_path = \"UNv1.0_en-zh\"\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# --- Extract tar.gz ---\n",
        "with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "    tar.extractall(path=extract_path)\n",
        "\n",
        "ch_file = os.path.join(extract_path, \"en-zh\", \"UNv1.0.en-zh.zh\")\n",
        "en_file = os.path.join(extract_path, \"en-zh\", \"UNv1.0.en-zh.en\")\n",
        "\n",
        "# --- Load sentences ---\n",
        "with open(ch_file, \"r\", encoding=\"utf-8\") as f_ch, open(en_file, \"r\", encoding=\"utf-8\") as f_en:\n",
        "    ch_lines = [line.strip() for line in f_ch]\n",
        "    en_lines = [line.strip() for line in f_en]\n",
        "\n",
        "df_un = pd.DataFrame({\"chinese\": ch_lines, \"english\": en_lines})\n",
        "print(f\"Total sentence pairs: {len(df_un)}\")\n",
        "\n",
        "# --- Cleaning ---\n",
        "def is_valid_sentence(s):\n",
        "    if len(s.split()) < 3:  # very short\n",
        "        return False\n",
        "    if len(s.split()) > 100:  # very long\n",
        "        return False\n",
        "    if re.fullmatch(r'[\\d\\(\\)\\-/\\s]+', s):  # headings/numbers only\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "df_un[\"english\"] = df_un[\"english\"].str.lower().str.strip()\n",
        "df_un[\"chinese\"] = df_un[\"chinese\"].str.strip()\n",
        "df_un = df_un[df_un[\"english\"].apply(is_valid_sentence)].reset_index(drop=True)\n",
        "print(f\"Sentence pairs after filtering: {len(df_un)}\")\n",
        "\n",
        "# --- Sample a reasonable subset for testing ---\n",
        "subset_size = 50000  # adjust as needed\n",
        "if len(df_un) > subset_size:\n",
        "    df_un = df_un.sample(subset_size, random_state=42).reset_index(drop=True)\n",
        "print(f\"Subset size for testing: {len(df_un)}\")\n",
        "\n",
        "df_un.to_csv(\"UN_cleaned.csv\", index=False, encoding=\"utf-8-sig\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3YpnvFcd59h"
      },
      "source": [
        "##IdiomKB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bc7KdqvPaK_5"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Load IdiomKB JSON, remove duplicates, save cleaned dataset\n",
        "# ==============================\n",
        "!pip install zhconv\n",
        "import pandas as pd\n",
        "import json\n",
        "from zhconv import convert\n",
        "import os\n",
        "\n",
        "# --- Step 0: Ensure we're in the repo folder ---\n",
        "# Adjust if your notebook is opened elsewhere\n",
        "REPO_PATH = \"/content/drive/MyDrive/chineseproverbs\"\n",
        "os.chdir(REPO_PATH)\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "# --- Step 1: Load JSON dataset (IdiomKB) ---\n",
        "with open('zh_idiom_meaning.json', 'r', encoding='utf-8') as f:\n",
        "    json_data = json.load(f)\n",
        "\n",
        "df_json = pd.DataFrame(json_data)\n",
        "df_json = df_json[['idiom', 'en_meaning']]\n",
        "df_json.rename(columns={'idiom': 'chinese', 'en_meaning': 'english'}, inplace=True)\n",
        "df_json['source'] = 'JSON'  # mark source\n",
        "\n",
        "print(f\"IdiomKB JSON dataset loaded: {len(df_json)} rows\")\n",
        "\n",
        "# --- Step 2: Normalize Chinese characters (Traditional -> Simplified) ---\n",
        "df_json['chinese'] = df_json['chinese'].apply(lambda x: convert(x, 'zh-cn'))\n",
        "\n",
        "# --- Step 3: Find and show duplicates ---\n",
        "duplicates = df_json[df_json.duplicated(subset='chinese', keep=False)]\n",
        "if not duplicates.empty:\n",
        "    print(\"\\nFound duplicates (before dropping):\")\n",
        "    print(duplicates.sort_values('chinese'))\n",
        "    print(f\"Total duplicates found: {len(duplicates)}\")\n",
        "else:\n",
        "    print(\"\\nNo duplicates found.\")\n",
        "\n",
        "# --- Step 4: Remove duplicates ---\n",
        "df_json.drop_duplicates(subset='chinese', keep='first', inplace=True)\n",
        "print(f\"Dataset after removing duplicates: {len(df_json)} rows\")\n",
        "\n",
        "# --- Step 5: Save cleaned dataset ---\n",
        "df_json.to_csv('idiomkb_cleaned.csv', index=False, encoding='utf-8-sig')\n",
        "print(\"\\nCleaned dataset saved to 'idiomkb_cleaned.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4RGgRbvkamZ"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Further clean English references and save refs_list\n",
        "# ==============================\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# --- Step 1: Load previously cleaned CSV ---\n",
        "df = pd.read_csv('idiomkb_cleaned.csv')\n",
        "print(f\"Loaded cleaned dataset: {len(df)} rows\")\n",
        "\n",
        "# --- Step 2: Further clean English references ---\n",
        "def clean_refs(text):\n",
        "    \"\"\"\n",
        "    Returns a list of cleaned English references:\n",
        "    - Lowercase and strip\n",
        "    - Replace first '(' with ',' and remove all ')'\n",
        "    - Strip leading/trailing quotes\n",
        "    - If quotes exist, only keep quoted strings\n",
        "    - Else if semicolons exist, split by semicolons\n",
        "    - Do NOT include original string if multiple references extracted\n",
        "    - Only include original string if nothing else extracted\n",
        "    \"\"\"\n",
        "    text = str(text).lower().strip()\n",
        "\n",
        "    # --- Minimal change: handle parentheses ---\n",
        "    text = re.sub(r'\\(', ',', text, count=1)  # first '(' -> ','\n",
        "    text = text.replace(')', '')              # remove all ')'\n",
        "\n",
        "    # 1️⃣ Extract quoted alternatives\n",
        "    quote_pattern = re.findall(r'\"([^\"]+)\"', text)\n",
        "    if quote_pattern:\n",
        "        parts = [q.strip().strip('\"').strip(\"'\") for q in quote_pattern]\n",
        "    # 2️⃣ Else split by semicolons\n",
        "    elif ';' in text:\n",
        "        parts = [p.strip().strip('\"').strip(\"'\") for p in text.split(';') if p.strip()]\n",
        "    # 3️⃣ Fallback: keep full original\n",
        "    else:\n",
        "        parts = [text.strip().strip('\"').strip(\"'\")]\n",
        "\n",
        "    # 4️⃣ Remove duplicates while preserving order\n",
        "    seen = set()\n",
        "    cleaned = []\n",
        "    for p in parts:\n",
        "        if p not in seen:\n",
        "            cleaned.append(p)\n",
        "            seen.add(p)\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "# --- Step 3: Apply to all rows ---\n",
        "df['refs_list'] = df['english'].apply(clean_refs)\n",
        "\n",
        "# --- Step 4: Save new cleaned dataset with references ---\n",
        "df.to_csv('idiomkb_cleaned_refs.csv', index=False, encoding='utf-8-sig')\n",
        "print(\"Further cleaned dataset with reference lists saved to 'idiomkb_cleaned_refs.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv6PE8skeKmV"
      },
      "source": [
        "### Split IdiomKB data (80:10:10)- Train: 6904, Validation: 864, Test: 864\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVknYEw9eADs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load cleaned dataset\n",
        "df = pd.read_csv(\"idiomkb_cleaned_refs.csv\")\n",
        "print(f\"Total dataset size: {len(df)}\")\n",
        "\n",
        "# Split: 80% train, 10% validation, 10% test\n",
        "train_val, test = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
        "train, val = train_test_split(train_val, test_size=0.1111, random_state=42)  # 0.1111*0.9 ≈ 0.1 total\n",
        "\n",
        "print(f\"Train: {len(train)}, Validation: {len(val)}, Test: {len(test)}\")\n",
        "\n",
        "# Save to separate CSVs\n",
        "train.to_csv(\"idiomkb_train.csv\", index=False, encoding=\"utf-8\")\n",
        "val.to_csv(\"idiomkb_val.csv\", index=False, encoding=\"utf-8\")\n",
        "test.to_csv(\"idiomkb_test.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"Saved train, validation, and test CSVs successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load opus100 dataset"
      ],
      "metadata": {
        "id": "cIHTKfY5DEzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load cleaned OPUS100 dataset under a different name\n",
        "opus_df = pd.read_csv(\"opus100_cleaned.csv\")\n",
        "print(f\"Total OPUS100 dataset size: {len(opus_df)}\")"
      ],
      "metadata": {
        "id": "EPUG5HlQDIIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZPdWZLanx6q"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skb-ql-snx6p"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIECoC03nx6r"
      },
      "outputs": [],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-zh-en\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOjurf0Nnx6s"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBcwEZMInx6s"
      },
      "source": [
        "## Translate some text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnGWExRLnx6s"
      },
      "outputs": [],
      "source": [
        "input_text = \"一举两得\"  # Chinese text you want to translate\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs)\n",
        "translated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(translated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6iB03yg5xlX"
      },
      "source": [
        "Testing Model Behavior for Traditional Characters\n",
        "\n",
        "Notes: same translation for both traditional & normal characters; neither captures meaningful idiom info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GUrJnLnj52y7"
      },
      "outputs": [],
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "model_name = \"Helsinki-NLP/opus-mt-zh-en\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "examples = [\"畫蛇添足\", \"画蛇添足\"]  # Traditional vs simplified\n",
        "\n",
        "for text in examples:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs)\n",
        "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"{text} -> {translation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh-vJtNPeo3V"
      },
      "source": [
        "#Model Evaluation (before Finetuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxFPt5-kevmV"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
        "\n",
        "# Tokenize Chinese input sentences\n",
        "test_inputs = tokenizer(list(test['chinese']), return_tensors='pt', padding=True, truncation=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEWt2A7jfU6C"
      },
      "source": [
        "## Behavior checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHevtAFafHTx"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Fast BLEU evaluation on test set (SacreBLEU + batching)\n",
        "# ==============================\n",
        "!pip install sacrebleu\n",
        "!pip install bleurt-pytorch\n",
        "\n",
        "import pandas as pd\n",
        "import ast\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import sacrebleu\n",
        "\n",
        "# --- Load test split (already cleaned CSV) ---\n",
        "test = pd.read_csv('idiomkb_cleaned_refs.csv')\n",
        "test['refs_list'] = test['refs_list'].apply(ast.literal_eval)\n",
        "\n",
        "# --- Setup GPU ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# --- Load model & tokenizer ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
        "model.to(device)\n",
        "\n",
        "# --- Prepare test sentences ---\n",
        "test_sentences = list(test['chinese'])\n",
        "\n",
        "# --- Generate translations in batches ---\n",
        "batch_size = 64\n",
        "translations = []\n",
        "\n",
        "for i in range(0, len(test_sentences), batch_size):\n",
        "    batch_texts = test_sentences[i:i+batch_size]\n",
        "    batch_inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True)\n",
        "    batch_inputs = {k:v.to(device) for k,v in batch_inputs.items()}\n",
        "    outputs = model.generate(**batch_inputs, max_length=64)\n",
        "    translations.extend([tokenizer.decode(t, skip_special_tokens=True).lower().strip() for t in outputs])\n",
        "\n",
        "print(f\"Generated {len(translations)} translations.\")\n",
        "\n",
        "# --- Prepare references for SacreBLEU ---\n",
        "# SacreBLEU expects list of predictions and list of reference lists (one per reference)\n",
        "references_clean = [[r.strip().strip('\"').strip(\"'\").lower() for r in ref_list]\n",
        "                    for ref_list in test['refs_list']]\n",
        "\n",
        "# SacreBLEU expects refs as list of lists per reference\n",
        "# If multiple references per sentence: [[ref1_sent1, ref1_sent2], [ref2_sent1, ref2_sent2], ...]\n",
        "refs_for_sacrebleu = list(zip(*references_clean))  # transpose to match SacreBLEU format\n",
        "\n",
        "# --- Inspect first 5 translations ---\n",
        "for src, refs, pred in zip(test['chinese'][:5], references_clean[:5], translations[:5]):\n",
        "    print(\"\\nSRC:\", src)\n",
        "    print(\"REFS:\", refs)\n",
        "    print(\"PRED:\", pred)\n",
        "    print(\"---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53r6WtVKq-mM"
      },
      "source": [
        "##Bleu score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "!pip install sacrebleu\n",
        "import sacrebleu\n",
        "\n",
        "def evaluate_bleu(df, source_col='chinese', refs_col='refs_list',\n",
        "                  model_name=\"Helsinki-NLP/opus-mt-zh-en\",\n",
        "                  batch_size=64, device=None, max_length=64):\n",
        "    \"\"\"\n",
        "    Evaluate SacreBLEU for a given dataset and seq2seq model.\n",
        "\n",
        "    Args:\n",
        "        df: pd.DataFrame containing source sentences and reference translations\n",
        "        source_col: column name of source sentences\n",
        "        refs_col: column name containing references (as list of strings)\n",
        "        model_name: Hugging Face model name\n",
        "        batch_size: batch size for generation\n",
        "        device: 'cuda', 'cpu', or None (auto-detect)\n",
        "        max_length: max length of generated sequences\n",
        "\n",
        "    Returns:\n",
        "        bleu_score: float BLEU score\n",
        "        translations: list of generated predictions\n",
        "    \"\"\"\n",
        "    # Setup device\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    model.to(device)\n",
        "\n",
        "    # Prepare test sentences\n",
        "    test_sentences = list(df[source_col])\n",
        "\n",
        "    # Generate translations in batches\n",
        "    translations = []\n",
        "    for i in range(0, len(test_sentences), batch_size):\n",
        "        batch_texts = test_sentences[i:i+batch_size]\n",
        "        batch_inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True)\n",
        "        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
        "        outputs = model.generate(**batch_inputs, max_length=max_length)\n",
        "        translations.extend([tokenizer.decode(t, skip_special_tokens=True).lower().strip() for t in outputs])\n",
        "\n",
        "    print(f\"Generated {len(translations)} translations.\")\n",
        "\n",
        "    # Prepare references for SacreBLEU\n",
        "    # Convert stringified lists if necessary\n",
        "    refs_lists = []\n",
        "    for r in df[refs_col]:\n",
        "        if isinstance(r, str):\n",
        "            refs_lists.append(ast.literal_eval(r))\n",
        "        else:\n",
        "            refs_lists.append(r)\n",
        "\n",
        "    references_clean = [[ref.strip().strip('\"').strip(\"'\").lower() for ref in ref_list]\n",
        "                        for ref_list in refs_lists]\n",
        "\n",
        "    # Transpose to match SacreBLEU expected input: list of lists per reference\n",
        "    refs_for_sacrebleu = list(zip(*references_clean))\n",
        "\n",
        "    # Compute BLEU\n",
        "    bleu = sacrebleu.corpus_bleu(translations, refs_for_sacrebleu)\n",
        "    print(f\"SacreBLEU score: {bleu.score:.2f}\")\n",
        "\n",
        "    return bleu.score, translations\n",
        "\n"
      ],
      "metadata": {
        "id": "Uq70geUzFNGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieXuEPglq9_r"
      },
      "outputs": [],
      "source": [
        "# --- Compute BLEU ---\n",
        "bleu = sacrebleu.corpus_bleu(translations, refs_for_sacrebleu)\n",
        "print(f\"\\nSacreBLEU score on test set: {bleu.score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score_opus, preds_opus = evaluate_bleu(opus_df, source_col='chinese', refs_col='refs_list')"
      ],
      "metadata": {
        "id": "2VPgMKIiFPeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bleurt"
      ],
      "metadata": {
        "id": "BHBxvUNNDPWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bleurt_pytorch import BleurtConfig, BleurtForSequenceClassification, BleurtTokenizer\n",
        "\n",
        "# Load BLEURT model (this will download ~1.6GB on first run)\n",
        "bleurt_config = BleurtConfig.from_pretrained('lucadiliello/BLEURT-20-D12')\n",
        "bleurt_model = BleurtForSequenceClassification.from_pretrained('lucadiliello/BLEURT-20-D12')\n",
        "bleurt_tokenizer = BleurtTokenizer.from_pretrained('lucadiliello/BLEURT-20-D12')\n",
        "\n",
        "# Move to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "bleurt_model.to(device)\n",
        "bleurt_model.eval()\n",
        "\n",
        "print(f\"BLEURT model loaded on {device}\")"
      ],
      "metadata": {
        "id": "vD5i0nWUCVIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_bleurt(predictions, references, batch_size=16, device=None):\n",
        "    \"\"\"\n",
        "    Evaluate BLEURT scores between predictions and references.\n",
        "\n",
        "    Args:\n",
        "        predictions: list of predicted translations\n",
        "        references: list of reference translations (one per prediction)\n",
        "        batch_size: batch size for BLEURT scoring\n",
        "        device: 'cuda', 'cpu', or None (auto-detect)\n",
        "\n",
        "    Returns:\n",
        "        mean_bleurt: average BLEURT score\n",
        "        bleurt_scores: list of individual BLEURT scores\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    bleurt_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(predictions), batch_size), desc=\"Computing BLEURT\"):\n",
        "            batch_preds = predictions[i:i+batch_size]\n",
        "            batch_refs = references[i:i+batch_size]\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = bleurt_tokenizer(\n",
        "                batch_refs,\n",
        "                batch_preds,\n",
        "                padding='longest',\n",
        "                return_tensors='pt',\n",
        "                max_length=512,\n",
        "                truncation=True\n",
        "            )\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            # Get BLEURT scores\n",
        "            outputs = bleurt_model(**inputs)\n",
        "            scores = outputs.logits.flatten().tolist()\n",
        "            bleurt_scores.extend(scores)\n",
        "\n",
        "    mean_bleurt = sum(bleurt_scores) / len(bleurt_scores)\n",
        "    print(f\"Mean BLEURT score: {mean_bleurt:.4f}\")\n",
        "\n",
        "    return mean_bleurt, bleurt_scores"
      ],
      "metadata": {
        "id": "r5tJ_GklDMH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation (BLEU, BertScore, BLEURT)"
      ],
      "metadata": {
        "id": "6pggQrPrEARa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import sacrebleu\n",
        "from bert_score import score as bert_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_all_metrics(df, source_col='chinese', refs_col='refs_list',\n",
        "                         model_name=\"Helsinki-NLP/opus-mt-zh-en\",\n",
        "                         batch_size=64, device=None, max_length=64,\n",
        "                         bertscore_model=\"microsoft/deberta-xlarge-mnli\",\n",
        "                         bleurt_batch_size=16):\n",
        "    \"\"\"\n",
        "    Generate translations and evaluate SacreBLEU, BERTScore, and BLEURT.\n",
        "\n",
        "    Returns:\n",
        "        metrics: dict with keys 'sacrebleu', 'bertscore_f1', 'bleurt'\n",
        "        translations: list of generated translations\n",
        "    \"\"\"\n",
        "    # Setup device\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    # Load translation model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    model.to(device)\n",
        "\n",
        "    # Prepare source sentences\n",
        "    source_texts = list(df[source_col])\n",
        "\n",
        "    # Generate translations in batches\n",
        "    translations = []\n",
        "    print(\"Generating translations...\")\n",
        "    for i in tqdm(range(0, len(source_texts), batch_size)):\n",
        "        batch_texts = source_texts[i:i+batch_size]\n",
        "        batch_inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True)\n",
        "        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
        "        outputs = model.generate(**batch_inputs, max_length=max_length)\n",
        "        translations.extend([tokenizer.decode(t, skip_special_tokens=True).strip().lower() for t in outputs])\n",
        "\n",
        "    print(f\"Generated {len(translations)} translations.\")\n",
        "\n",
        "    # Prepare references\n",
        "    refs_lists = []\n",
        "    for r in df[refs_col]:\n",
        "        if isinstance(r, str):\n",
        "            refs_lists.append(ast.literal_eval(r))\n",
        "        else:\n",
        "            refs_lists.append(r)\n",
        "\n",
        "    references_clean = [[ref.strip().strip('\"').strip(\"'\").lower() for ref in ref_list]\n",
        "                       for ref_list in refs_lists]\n",
        "\n",
        "    # SacreBLEU\n",
        "    print(\"\\nComputing SacreBLEU...\")\n",
        "    refs_for_sacrebleu = list(zip(*references_clean))\n",
        "    bleu = sacrebleu.corpus_bleu(translations, refs_for_sacrebleu)\n",
        "    sacrebleu_score = bleu.score\n",
        "\n",
        "    # BERTScore (use first reference per sentence)\n",
        "    print(\"Computing BERTScore...\")\n",
        "    first_refs = [refs[0] for refs in references_clean]\n",
        "    P, R, F1 = bert_score(translations, first_refs,\n",
        "                         model_type=bertscore_model,\n",
        "                         lang=\"en\",\n",
        "                         rescale_with_baseline=True,\n",
        "                         device=device)\n",
        "    bertscore_f1 = F1.mean().item()\n",
        "\n",
        "    # BLEURT (use first reference per sentence)\n",
        "    print(\"Computing BLEURT...\")\n",
        "    bleurt_mean, bleurt_scores = evaluate_bleurt(\n",
        "        translations,\n",
        "        first_refs,\n",
        "        batch_size=bleurt_batch_size,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    metrics = {\n",
        "        \"sacrebleu\": sacrebleu_score,\n",
        "        \"bertscore_f1\": bertscore_f1,\n",
        "        \"bleurt\": bleurt_mean\n",
        "    }\n",
        "\n",
        "    return metrics, translations, bleurt_scores"
      ],
      "metadata": {
        "id": "vOi3MzsPGHMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Idiom dataset\n",
        "print(\"=\"*50)\n",
        "print(\"Evaluating on Idiom dataset\")\n",
        "print(\"=\"*50)\n",
        "test_df = pd.read_csv('idiomkb_test.csv')\n",
        "metrics, preds, bleurt_scores = evaluate_all_metrics(test_df)\n",
        "print(\"\\n--- Idiom Dataset Results ---\")\n",
        "for key, value in metrics.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "# OPUS100 dataset\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Evaluating on OPUS100 dataset\")\n",
        "print(\"=\"*50)\n",
        "opus_df = pd.read_csv('opus100_cleaned.csv')\n",
        "metrics_opus, preds_opus, bleurt_scores_opus = evaluate_all_metrics(opus_df)\n",
        "print(\"\\n--- OPUS100 Dataset Results ---\")\n",
        "for key, value in metrics_opus.items():\n",
        "    print(f\"{key}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "5uPLOt-OGJUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Plot BLEURT score distribution\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(bleurt_scores, bins=50, edgecolor='black', alpha=0.7)\n",
        "plt.xlabel('BLEURT Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Idiom Dataset BLEURT Distribution')\n",
        "plt.axvline(np.mean(bleurt_scores), color='red', linestyle='--', label=f'Mean: {np.mean(bleurt_scores):.3f}')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(bleurt_scores_opus, bins=50, edgecolor='black', alpha=0.7)\n",
        "plt.xlabel('BLEURT Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('OPUS100 BLEURT Distribution')\n",
        "plt.axvline(np.mean(bleurt_scores_opus), color='red', linestyle='--', label=f'Mean: {np.mean(bleurt_scores_opus):.3f}')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show examples with lowest and highest BLEURT scores\n",
        "print(\"\\n=== Idiom Dataset: Lowest BLEURT Scores ===\")\n",
        "lowest_indices = np.argsort(bleurt_scores)[:5]\n",
        "for idx in lowest_indices:\n",
        "    print(f\"\\nBLEURT: {bleurt_scores[idx]:.3f}\")\n",
        "    print(f\"Source: {test_df.iloc[idx]['chinese']}\")\n",
        "    print(f\"Reference: {ast.literal_eval(test_df.iloc[idx]['refs_list'])[0]}\")\n",
        "    print(f\"Prediction: {preds[idx]}\")\n",
        "\n",
        "print(\"\\n=== Idiom Dataset: Highest BLEURT Scores ===\")\n",
        "highest_indices = np.argsort(bleurt_scores)[-5:]\n",
        "for idx in highest_indices:\n",
        "    print(f\"\\nBLEURT: {bleurt_scores[idx]:.3f}\")\n",
        "    print(f\"Source: {test_df.iloc[idx]['chinese']}\")\n",
        "    print(f\"Reference: {ast.literal_eval(test_df.iloc[idx]['refs_list'])[0]}\")\n",
        "    print(f\"Prediction: {preds[idx]}\")"
      ],
      "metadata": {
        "id": "ZIxuH69NDwlQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}